{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking1:什么是监督学习，无监督学习，半监督学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 监督学习\n",
    "- 带有标签的学习\n",
    "2. 无监督学习\n",
    "- 没有标签的学习\n",
    "3. 半监督学习（Semi-Supervised learning):\n",
    "- 半监督学习介于监督学习和无监督学习之间\n",
    "- 通常半监督学习的任务与监督学习一致，即任务中包含有明确的目标（如分类），采用的数据包括有标签的数据，也包括无标签的数据\n",
    "- 作用：只有少量的数据有 Label,利用没有标签的数据来学习整个数据的潜在分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking2:K-means 中的k值如何选取？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据的先验知识，或者数据进行简单的分析\n",
    "- 基于变化的算法:给定一个合理的类簇指标，比如平均半径或者直径，只要我们假设的类簇的数目等于或者高于真实的类簇的数目时，该指标上升缓慢，而一旦试图得到少于真实数目的类簇时，该指标会急剧上升。\n",
    "- 基于结构的算法：即比较类内距离、类间距离以确定K\n",
    "- 基于一致性矩阵的算法\n",
    "- 基于层次聚合\n",
    "- 基于采样的算法\n",
    "- 使用Canopy Method 算法初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking3:随机森林采用了 bagging 集成学习，bagging 指的是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将若干个弱分类器的分类结果进行投票选择，从而组成一个强分类器，这就是随机森林的 bagging 思想,实际上bagging的思想是’机器学习加强版‘。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking4:表征学习和半监督学习的区别是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 表征学习：\n",
    "- 定义：表征学习（representation),也称为特征学习（feature learning），目的是对复杂的原始数据化繁为简，把原始的无效信息剔除，把有效信息更有效地进行提炼，形成特征\n",
    "\n",
    "\n",
    "\n",
    "2. 半监督学习：\n",
    "- 定义：通常半监督学习的任务与监督学习一致，即任务中包含有明确的目标（如分类），采取的数据既包括有标签的数据，也包括无标签的数据\n",
    "- 作用：只有少量的数据有Label,利用没有标签的数据来学习整个数据的潜在分布\n",
    "\n",
    "表征学习针对的是整个数据集，而半监督学习针对的是对没有标签的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本抄袭自动检测分析\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os \n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Normalizer  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import jieba\n",
    "import editdistance\n",
    "import pickle  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载停用词\n",
    "with open('chinese_stopwords.txt','r',encoding = 'utf-8') as file:\n",
    "    stopwords = [i[:-1] for i in file.readlines()]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "news = pd.read_csv('sqlResult.csv',encoding = 'gb18030')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理缺失值\n",
    "news = news.dropna(subset = ['content'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词（在写 TF-IDF之前需要进行一下分词）\n",
    "def split_text(text):\n",
    "    text = text.replace(' ','')  \n",
    "    text = text.replace('\\n','')  \n",
    "    text2 = jieba.cut(text.strip())  \n",
    "    result = ' '.join([w for w in text2 if w not in stopwords])  \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('corpus.pkl'):\n",
    "    corpus = list(map(split_text,[str(i) for i in news.content]))  \n",
    "    with open('corpus.pkl','wb') as file: \n",
    "        pickle.dump(corpus,file) \n",
    "else:\n",
    "    with open('corpus.pkl','rb') as file:\n",
    "        corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算 corpus 的 TF-IDF矩阵\n",
    "countvectorizer = CountVectorizer(encoding = 'gb18030',min_df = 0.015) \n",
    "tfidftransformer = TfidfTransformer()\n",
    "countvector = countvectorizer.fit_transform(corpus) \n",
    "tfidf = tfidftransformer.fit_transform(countvector)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自己标记是否是自己的新闻\n",
    "label = list(map(lambda source: 1 if '新华' in str(source) else 0,news.source))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可疑文章数： 2828\n"
     ]
    }
   ],
   "source": [
    "# 数据集划分\n",
    "x_train,x_test,y_train,y_test = train_test_split(tfidf.toarray(),label,test_size = 0.3,random_state = 33) # random_state 是随机数种子，这个值可以自己设置\n",
    "\n",
    "#  定义一个多项式的朴素贝叶斯分类器\n",
    "clf = MultinomialNB()\n",
    "clf.fit(x_train,y_train)\n",
    "prediction = clf.predict(tfidf.toarray())  # 这里的 tfidf.toarray是全量数据\n",
    "\n",
    "labels = np.array(label)  \n",
    "compare_news_index = pd.DataFrame({'prediction':prediction,'labels':labels}) \n",
    "# 计算所有可疑文章的 index \n",
    "copy_news_index = compare_news_index[(compare_news_index['prediction'] == 1) & (compare_news_index['labels']  == 0)].index\n",
    "print('可疑文章数：',len(copy_news_index))\n",
    "# 计算所有新华社文章的 index\n",
    "xinhuashe_news_index = compare_news_index[(compare_news_index['labels'] == 1)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "scaled_array = normalizer.fit_transform(tfidf.toarray())  \n",
    "\n",
    "if not os.path.exists('label.pkl'):\n",
    "    # 使用 kmeans ,对全量文档做聚类\n",
    "    kmeans = KMeans(n_clusters = 25)  \n",
    "    k_labels = kmeans.fit_predict(scaled_array)\n",
    "    with open('label.pkl','wb') as file:\n",
    "        pickle.dump(k_labels,file)\n",
    "    print('k_labels.shape',k_labels.shape)\n",
    "else:\n",
    "    with open('label.pkl','rb') as file:\n",
    "        k_labels = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建反向的 id_class\n",
    "if not os.path.exists('id_class.pkl'):\n",
    "    id_class = {index:class_ for index,class_ in enumerate(k_labels)}\n",
    "    with open('id_class.pkl','wb') as file:\n",
    "        pickle.dump(id_class,file)\n",
    "else:\n",
    "    with open('id_class.pkl','rb') as file:\n",
    "        id_class = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建反向的 class_id\n",
    "if not os.path.exists('class_id.pkl'):\n",
    "    class_id = defaultdict(set)  \n",
    "    for index,class_ in id_class.items():\n",
    "        if index in xinhuashe_news_index.tolist():\n",
    "            class_id[class_].add(index)\n",
    "    with open('class_id.pkl','wb') as file:\n",
    "        pickle.dump(class_id,file)\n",
    "else:\n",
    "    with open('class_id.pkl','rb') as file:\n",
    "        class_id = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 找相似文本\n",
    "def find_similar_text(cpindex,top = 10):\n",
    "    dist_dict = {i:cosine_similarity(tfidf[cpindex],tfidf[i]) for i in class_id[id_class[cpindex]]}\n",
    "    return sorted(dist_dict.items(),key = lambda x:x[1][0],reverse = True)[:top] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 统计可疑抄袭文本里面的编辑距离 \n",
    "dict_ = dict()\n",
    "for cpindex in copy_news_index:\n",
    "    similar_list = find_similar_text(cpindex)\n",
    "    # 找一篇相似的原文\n",
    "    similar2 = similar_list[0][0]\n",
    "    # 参看编辑距离\n",
    "    editdistance_ = editdistance.eval(corpus[cpindex],corpus[similar2])\n",
    "    dict_[cpindex] = (editdistance_,similar2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{7968: 46172, 5709: 86644, 2172: 2246, 3794: 1115, 8262: 8447}\n"
     ]
    }
   ],
   "source": [
    "# 把所有可疑抄袭的文章按照 计算出来的编辑距离从小到大进行排序\n",
    "dict_new = sorted(dict_.items(),key = lambda d:d[1][0])\n",
    "most_similarity_index = {x[0]:x[1][1] for x in dict_new[:5]}\n",
    "print(most_similarity_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看编码距离最小的五项，即最可能发生抄袭的五篇\n",
    "for cpindex,similar2 in most_similarity_index.items():\n",
    "\n",
    "    print('怀疑抄袭:\\n',news.iloc[cpindex].content)\n",
    "    # 找一篇相似的原文\n",
    "    print('相似原文\\n',news.iloc[similar2].content)\n",
    "    # 参看编辑距离\n",
    "    print('编辑距离:', dict_[cpindex][0])\n",
    "    print('**********************************************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
